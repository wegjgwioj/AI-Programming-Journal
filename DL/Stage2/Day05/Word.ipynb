{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本分类词嵌入（Word2Vec）、LSTM、Attention机制\n",
    "\n",
    "- 学习目标：掌握使用词嵌入（Word2Vec）、LSTM 和 Attention 机制进行文本分类的技术，并在 IMDB 电影评论情感分析任务中实现准确率 > 85%。\n",
    "- 学习内容：\n",
    "  - 理解词嵌入（Word2Vec）的基本原理和实现方法。\n",
    "  - 学习 LSTM 网络结构及其在文本分类中的应用。\n",
    "  - 掌握 Attention 机制的原理，并将其应用于文本分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词嵌入（Word2Vec）\n",
    "\n",
    "词嵌入是一种将词语转换为向量表示的方法，使得相似的词语在向量空间中距离较近。Word2Vec 是一种常用的词嵌入方法，它通过训练神经网络来学习词语的向量表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例代码：训练 Word2Vec 模型\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 假设 sentences 是一个包含文本数据的列表，每个元素是一个句子（词语列表）\n",
    "sentences = [['this', 'is', 'a', 'sample', 'sentence'], ['another', 'example', 'sentence']]\n",
    "\n",
    "# 训练 Word2Vec 模型\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 获取词语 'sample' 的向量表示\n",
    "vector = model.wv['sample']\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM 网络结构\n",
    "\n",
    "LSTM（Long Short-Term Memory）是一种特殊的 RNN（Recurrent Neural Network），它通过引入门控机制来解决传统 RNN 中的梯度消失和梯度爆炸问题。LSTM 在处理序列数据（如文本）时表现出色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例代码：构建 LSTM 模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # 词嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM 层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        # Dropout 层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 嵌入输入\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # LSTM 输出\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # 全连接层输出\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention 机制\n",
    "\n",
    "Attention 机制是一种增强模型对重要信息关注能力的方法。它通过为输入序列中的每个元素分配一个权重，从而使模型能够更好地捕捉全局信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例代码：在 LSTM 模型中加入 Attention 机制\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, lstm_out):\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        return context\n",
    "\n",
    "class LSTMModelWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(LSTMModelWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.attention = Attention(hidden_dim * 2 if bidirectional else hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        context = self.attention(lstm_out)\n",
    "        output = self.fc(context)\n",
    "        return output\n",
    "'''\n",
    "传统 LSTM 和加入 Attention 机制的 LSTM 的区别：\n",
    "传统 LSTM 模型直接将 LSTM 层的最后一个时间步的输出传递给全连接层。\n",
    "加入 Attention 机制的 LSTM 模型通过计算每个时间步的注意力权重，得到上下文向量，再将上下文向量传递给全连接层。\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练和评估模型\n",
    "# ...existing code...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myc1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
