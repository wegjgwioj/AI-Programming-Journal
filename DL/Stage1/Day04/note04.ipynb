{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "第4天理解自动微分机制（PyTorch的backward或TF的GradientTape）手动实现简单线性回归"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfLnDUxeJyxp"
      },
      "source": [
        "自动微分（Automatic Differentiation，AD）是深度学习框架（如PyTorch和TensorFlow）的核心机制，用于自动计算梯度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RzTxOaCiJ7Hn"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prk0TFeHJ_LJ"
      },
      "source": [
        " # 基本思想\n",
        " **链式法则** 复杂函数梯度分解为原子操作的梯度组合。（追踪计算过程实现高效梯度计算）\n",
        "  * 前向模式 ：计算顺序逐层计算，适合输入维度低\n",
        "  * 反向模式： 从输出反向逐层累积梯度，适合输出维度低的情况（如损失函数），深度学习框架主要用此模式。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NI1c5nTK15v"
      },
      "source": [
        "# 2. PyTorch的自动微分（backward）\n",
        " PyTorch通过动态计算图（Dynamic Computation Graph）实现自动微分。\n",
        "\n",
        "###  追踪计算过程：\n",
        "\n",
        " * 当张量（Tensor）的requires_grad=True时，所有相关计算会被记录为计算图。\n",
        "\n",
        " * 每个操作生成一个Function节点（包含前向和反向的实现）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v-x2kwT4LLzn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x ** 2 + 3 * x  # 构建计算图"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf6R_fSULPca"
      },
      "source": [
        "### 反向传播\n",
        " * 调用y.backward()时，PyTorch从y开始反向遍历计算图。\n",
        "\n",
        " * 对每个节点应用链式法则，计算梯度并存储在.grad属性中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR7Im582LN1F",
        "outputId": "8e9bdefd-f58d-4283-d591-c37b7bbd2df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(7.)\n"
          ]
        }
      ],
      "source": [
        "y.backward()        # 反向传播\n",
        "print(x.grad)       # 输出梯度：2*2 + 3 = 7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVdAis_bNlbF"
      },
      "source": [
        "* 特性：\n",
        "\n",
        " * 动态图：每次前向传播都会构建新计算图，支持灵活控制流（如循环、条件）。\n",
        "\n",
        " * 梯度累加：默认梯度会累加，需手动清零（optimizer.zero_grad()）。\n",
        "\n",
        " * 非标量梯度：对向量输出需传入gradient参数（形状匹配）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb5KBdp6N3tC"
      },
      "source": [
        "# 3 TensorFlow的自动微分（GradientTape）\n",
        "\n",
        "TensorFlow通过tf.GradientTape上下文管理器记录计算过程，支持动态图和静态图。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btvEvpqBOAKl"
      },
      "source": [
        "###  关键步骤：\n",
        " * 记录计算过程：\n",
        "\n",
        "  * 在GradientTape上下文中执行的操作会被记录\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GnEoIAYWOQiq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x ** 2 + 3 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mptGPMfxOYBO"
      },
      "source": [
        "* 计算梯度：\n",
        " * 使用tape.gradient(target, sources)计算梯度。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D5csooCOmaS",
        "outputId": "37f6ecd0-059b-4d5c-b47b-091c88d267e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.0\n"
          ]
        }
      ],
      "source": [
        "dy_dx = tape.gradient(y, x)  # 梯度计算\n",
        "print(dy_dx.numpy())         # 输出：7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SEgcfzROq6l"
      },
      "source": [
        "* 特性：\n",
        "\n",
        "  * 默认只记录一次：通过persistent=True可多次计算梯度。\n",
        "\n",
        "  * 资源释放：非持久模式自动释放资源，避免内存泄漏。\n",
        "\n",
        "  * Eager Execution：默认启用动态图，与静态图兼容"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GEuP-iKPK2A"
      },
      "source": [
        "# 注意事项\n",
        " * 内存管理：\n",
        "  \n",
        "  反向传播后，PyTorch自动释放计算图；TensorFlow的GradientTape默认只记录一次。\n",
        "\n",
        " * 不可变操作：\n",
        "\n",
        " 某些操作（如+=）可能破坏梯度追踪，需使用x.assign()（TensorFlow）或避免原地操作。\n",
        "\n",
        " * 性能优化：\n",
        "\n",
        " 静态图（如@tf.function）可通过编译提高速度，但牺牲灵活性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaAJBHZoUtfz"
      },
      "source": [
        "# 手动实现线性回归模型（Pytorch）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H9iQXyQXnz2",
        "outputId": "780d355b-b8de-43ba-bf3e-1b8d853f4db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [100/1000], Loss: 0.0581, w: 1.4418, b: 0.7990\n",
            "Epoch [200/1000], Loss: 0.0571, w: 1.4542, b: 0.7707\n",
            "Epoch [300/1000], Loss: 0.0565, w: 1.4640, b: 0.7484\n",
            "Epoch [400/1000], Loss: 0.0562, w: 1.4717, b: 0.7309\n",
            "Epoch [500/1000], Loss: 0.0559, w: 1.4778, b: 0.7172\n",
            "Epoch [600/1000], Loss: 0.0558, w: 1.4825, b: 0.7064\n",
            "Epoch [700/1000], Loss: 0.0557, w: 1.4863, b: 0.6979\n",
            "Epoch [800/1000], Loss: 0.0556, w: 1.4892, b: 0.6912\n",
            "Epoch [900/1000], Loss: 0.0556, w: 1.4915, b: 0.6860\n",
            "Epoch [1000/1000], Loss: 0.0556, w: 1.4933, b: 0.6818\n",
            "Final w: 1.4933, b: 0.6818\n"
          ]
        }
      ],
      "source": [
        "# prompt: PyTorch手动实现简单线性回归\n",
        "\n",
        "import torch\n",
        "\n",
        "# 1. 准备数据\n",
        "X = torch.tensor([[1.0], [2.0], [3.0]])  # 特征\n",
        "y = torch.tensor([[2.0], [4.0], [5.0]])  # 目标变量\n",
        "\n",
        "# 2. 初始化参数\n",
        "w = torch.randn(1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "# 3. 设置学习率和迭代次数\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# 4. 训练模型\n",
        "for epoch in range(epochs):\n",
        "    # 计算预测值\n",
        "    y_pred = X * w + b\n",
        "\n",
        "    # 计算损失\n",
        "    loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # 反向传播计算梯度\n",
        "    loss.backward()\n",
        "\n",
        "    # 更新参数\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        w.grad.zero_()  # 清除梯度\n",
        "        b.grad.zero_()  # 清除梯度\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, w: {w.item():.4f}, b: {b.item():.4f}')\n",
        "\n",
        "# 5. 打印最终结果\n",
        "print(f'Final w: {w.item():.4f}, b: {b.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4W9wzFbY910"
      },
      "source": [
        "# TensorFlow手动实现简单线性回归（输出）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XahwqKoZAGQ",
        "outputId": "297f7310-9373-423a-f2fe-38f3a95a5d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [100/1000], Loss: 0.0584, w: 1.4381, b: 0.8073\n",
            "Epoch [200/1000], Loss: 0.0573, w: 1.4514, b: 0.7772\n",
            "Epoch [300/1000], Loss: 0.0566, w: 1.4618, b: 0.7536\n",
            "Epoch [400/1000], Loss: 0.0562, w: 1.4699, b: 0.7350\n",
            "Epoch [500/1000], Loss: 0.0560, w: 1.4764, b: 0.7204\n",
            "Epoch [600/1000], Loss: 0.0558, w: 1.4814, b: 0.7089\n",
            "Epoch [700/1000], Loss: 0.0557, w: 1.4854, b: 0.6999\n",
            "Epoch [800/1000], Loss: 0.0557, w: 1.4885, b: 0.6928\n",
            "Epoch [900/1000], Loss: 0.0556, w: 1.4910, b: 0.6872\n",
            "Epoch [1000/1000], Loss: 0.0556, w: 1.4929, b: 0.6828\n",
            "Final w: 1.4929, b: 0.6828\n"
          ]
        }
      ],
      "source": [
        "# prompt: TensorFlow手动实现简单线性回归\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1. 准备数据\n",
        "X = tf.constant([[1.0], [2.0], [3.0]], dtype=tf.float32)  # 特征\n",
        "y = tf.constant([[2.0], [4.0], [5.0]], dtype=tf.float32)  # 目标变量\n",
        "\n",
        "# 2. 初始化参数\n",
        "w = tf.Variable(tf.random.normal([1, 1]), dtype=tf.float32)\n",
        "b = tf.Variable(tf.random.normal([1, 1]), dtype=tf.float32)\n",
        "\n",
        "# 3. 设置学习率和迭代次数\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# 4. 训练模型\n",
        "for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # 计算预测值\n",
        "        y_pred = tf.matmul(X, w) + b\n",
        "\n",
        "        # 计算损失\n",
        "        loss = tf.reduce_mean(tf.square(y_pred - y))\n",
        "\n",
        "    # 计算梯度\n",
        "    dw, db = tape.gradient(loss, [w, b])\n",
        "\n",
        "    # 更新参数\n",
        "    w.assign_sub(learning_rate * dw)\n",
        "    b.assign_sub(learning_rate * db)\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.numpy():.4f}, w: {w.numpy()[0][0]:.4f}, b: {b.numpy()[0][0]:.4f}')\n",
        "\n",
        "# 5. 打印最终结果\n",
        "print(f'Final w: {w.numpy()[0][0]:.4f}, b: {b.numpy()[0][0]:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
