# stage 1

* 线代✅
* 概率论✅
* 微积分✅
* Python✅
* R
* CPP✅
* 机器学习基础 （torch 实现 线性回归，MNIST分类任务）✅

---

# stage2

### 模块1 CNN+RNN

1. **CNN（卷积神经网络）✅**

* 核心概念：卷积核、池化、感受野、特征图✅
* 经典结构：LeNet、ResNet（残差连接）✅
* **视觉应用** ：图像分类（CIFAR-10）✅

2. **RNN（循环神经网络）✅**

* 核心概念：时间步、隐藏状态、梯度消失✅
* 变体：LSTM（遗忘门）、GRU（简化版LSTM）✅
* **应用场景** ：时序预测、机器翻译（Seq2Seq）✅

---



* 用PyTorch实现TextCNN完成新闻分类（数据集：AG News）
* 手绘LSTM单元结构图（标注输入门、遗忘门、输出门）

---

### **模块2：Transformer架构**

1. **Self-Attention机制**
   * 数学推导：Q/K/V矩阵计算 → 注意力权重 → 上下文向量
   * 代码实现：手动编写Attention层（禁止使用库函数）
2. **Transformer整体架构**
   * 编码器：多头注意力 → Add & Norm → 前馈网络
   * 解码器：Masked多头注意力 → 交叉注意力
   * 位置编码：正弦函数 vs 可学习编码
3. **BERT与GPT对比**
   * BERT：双向Transformer，MLM+NSP预训练任务
   * GPT：单向Transformer，自回归语言模型

* **论文精读** ：
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)（必读）
* [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
* **可视化工具** ：
* [Transformer动画演示](https://jalammar.github.io/illustrated-transformer/)
* [BERT Feature可视化](https://exbert.net/)
* ✅ 从零实现Transformer Encoder（参考[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
* ✅ 对比BERT-base和GPT-2的模型结构差异（表格形式）

---

### **模块3：Hugging Face实战（4周）**

1. **Transformers库核心功能**
   * Pipeline快速调用：`pipeline("text-generation", model="gpt2")`
   * 微调自定义数据集：`Trainer` API + `Dataset` 类
   * 模型共享：上传至[Hugging Face Hub](https://huggingface.co/models)
2. **典型NLP任务**
   * 文本分类：Fine-tune BERT on IMDB影评
   * 序列标注：用BERT-CRF做命名实体识别（CoNLL-2003）
   * 文本生成：用GPT-2生成知乎风格回答
3. **部署优化**
   * 模型量化：使用 `optimum`库实现BERT动态量化
   * ONNX导出：`transformers.onnx`导出模型供跨平台推理

* **官方教程** ：
* [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter1/1)（免费，含代码环境）
* [Fine-tuning指南](https://huggingface.co/docs/transformers/training)
* **数据集** ：
* [GLUE Benchmark](https://gluebenchmark.com/)（多任务评估）
* [ChnSentiCorp](https://github.com/SophonPlus/ChineseNlpCorpus)（中文情感分析）
* ✅ 微调BERT模型实现微博情感分析（Accuracy > 90%）
* ✅ 将微调后的模型部署为FastAPI服务（示例代码见下方）

```
from fastapi import FastAPI
from transformers import pipeline

app = FastAPI()
classifier = pipeline("text-classification", model="my_bert_finetuned")

@app.post("/predict")
def predict(text: str):
    return classifier(text)
```

---

### **模块4：Kaggle竞赛冲刺**

1. **赛题选择** ：

* 入门推荐：[Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)（情感+实体抽取）
* 进阶挑战：[CommonLit Readability Prize](https://www.kaggle.com/c/commonlitreadabilityprize)（文本复杂度评估）

1. **技巧总结** ：

* 数据清洗：去除特殊字符、标准化缩写（如"don't" → "do not"）
* 模型融合：BERT + RoBERTa 预测结果加权平均
* 提分秘籍：Pseudo Labeling（用测试集伪标签扩充训练数据）
* **Kaggle实战模板** ：
* [NLP Starter Kit](https://www.kaggle.com/code/abhishek/nlp-starter-kit-bert-using-tf-and-pytorch)
* [Hugging Face集成示例](https://www.kaggle.com/code/debarshichanda/pytorch-bert-baseline)
* **效率工具** ：
* [Weights &amp; Biases](https://wandb.ai/)：实验追踪与超参数调优
* ✅ 提交至少3次有效结果，排名进入前50%
* ✅ 撰写竞赛总结（包括EDA、模型选择、错误分析）

---

### **避坑指南**

1. **不要陷入理论泥潭** ：遇到数学推导卡顿时，先跑通代码再回头理解。
2. **警惕过拟合** ：Kaggle竞赛中若Public LB分数高但Private LB暴跌，可能是数据泄露或验证集划分不当。
3. **模型选择** ：优先使用Hugging Face上已有预训练权重的模型（如 `bert-base-chinese`），避免从零训练。
