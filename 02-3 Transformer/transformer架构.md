
# [Attention Is All You Need](https://arxiv.org/abs/1706.03762)（必读论文）

[BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)

# Self-Attention机制

* 代码实现：to:  1.selfAttention.md
* [Self-Attention机制](/02-3 Transformer/1.selfAttention.md)

# 代码实现Self-Attention


# **Transformer整体架构**

**1. 多头注意力机制 (Multi-Head Attention)**

- 核心思想：通过并行化的多个"注意力头"，让模型同时关注输入序列不同位置的关联性
- 数学过程：
  - 输入向量被线性投影为 Q(Query), K(Key), V(Value) 三组向量
  - 每个注意力头计算：Attention(Q,K,V) =$ softmax(\frac{QK^T}{√d_k})V$
  - 多头结果拼接后通过线性层输出
- 优势：相比单头注意力，能捕捉更丰富的上下文依赖模式

**2. 残差连接与层归一化 (Add & Norm)**

- 残差连接（Add）：
  - 数学形式：Output = Sublayer(x) + x
  - 作用：缓解梯度消失，使深层网络更容易训练
- 层归一化（Norm）：
  - 对每个样本的特征维度进行标准化
  - 公式：LayerNorm(x) = γ*(x-μ)/σ + β
  - 作用：稳定训练过程，加速收敛

**3. 前馈网络 (Feed Forward Network)**

- 结构特点：
  - 两层的全连接神经网络
  - 中间维度通常扩大4倍（例如512→2048→512）
  - 使用ReLU等激活函数引入非线性
- 数学表达：FFN(x) = max(0, xW1 + b1)W2 + b2
- 功能：对注意力输出的综合表征进行空间变换和增强

**完整数据流示例**（假设输入维度d_model=512）：

1. 输入序列（batch_size, seq_len, 512）
2. 经过多头注意力（保留维度512）
3. 残差连接后层归一化（保持512维）
4. 前馈网络处理（维度变化：512→2048→512）
5. 再次进行残差连接和层归一化（最终输出512维）

**注**：标准Transformer编码器实际包含两个Add & Norm操作：
多头注意力 → Add & Norm → 前馈网络 → Add & Norm

这种架构设计使得：

- 每个子层（注意力/前馈）都能保持输入的主要信息
- 层归一化对梯度传播起到稳定作用
- 前馈网络提供强大的特征变换能力
- 总参数量约为：4*d_model^2 + 2*d_model*d_ff（d_ff通常为4*d_model）

# 从零实现Transformer Encoder（参考[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html))

# **BERT与GPT对比**

**1. 核心架构差异**

| **维度**       | **BERT**                         | **GPT**                                                  |
| -------------------- | -------------------------------------- | -------------------------------------------------------------- |
| **基础架构**   | 多层Transformer编码器堆叠              | 多层Transformer解码器堆叠<br />（仅保留masked self-attention） |
| **注意力方向** | 双向上下文建模（同时看左右token）      | 单向自回归建模（仅看左侧历史token）                            |
| **位置编码**   | 可学习的位置嵌入（Position Embedding） | 原始Transformer的正弦位置编码<br />（GPT-2/3后改为可学习）     |
| **参数量级**   | Base版约1.1亿参数（12层）              | GPT-3达1750亿参数（96层）                                      |

**2. 预训练任务对比**

| **任务类型**   | **BERT**                                                     | **GPT**                                           |
| -------------------- | ------------------------------------------------------------------ | ------------------------------------------------------- |
| **核心任务**   | Masked Language Model (MLM) + <br />Next Sentence Prediction (NSP) | 自回归语言建模（预测下一个token）                       |
| **数据利用率** | 通过MLM可同时利用上下文信息                                        | 只能利用历史信息，数据效率较低                          |
| **任务示例**   | 输入："The [MASK] sat on the mat" <br />→ 预测"cat"               | 输入："The cat sat" → 预测"on"；输入+"on" → 预测"the" |
| **训练稳定性** | MLM存在预训练-微调差异<br />（[MASK] token在微调时不存在）         | 任务形式完全一致，无模式差异                            |

**3. 解码器结构详解**

| **组件**       | **BERT**                 | **GPT**                                |
| -------------------- | ------------------------------ | -------------------------------------------- |
| **注意力机制** | 标准多头注意力（无mask）       | Masked多头注意力（防止信息泄露）             |
| **交叉注意力** | 无（纯编码器架构）             | 仅在完整Transformer解码器中出现（GPT不使用） |
| **层间连接**   | 每个编码器层包含两次Add & Norm | 解码器层结构与编码器相似，但注意力机制不同   |

**4. 位置编码演进**

| **编码类型** | **BERT实现**                                         | **原始Transformer/GPT**                                                                       |
| ------------------ | ---------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| **数学形式** | 可学习参数：$E_{pos} \in \mathbb{R}^{max\_len \times d}$ | 正弦函数：$PE(pos,2i)=sin(pos/10000^{2i/d})$                  $PE(pos,2i+1)=cos(...)$ |
| **优势**     | 灵活适应不同序列长度                                       | 理论可外推到任意长度                                                                                |
| **实践表现** | 实际工程中差异不大，可学习编码成为主流                     | GPT后续版本转向可学习编码                                                                           |

**5. 应用场景差异**

| **场景**       | **BERT优势场景**             | **GPT优势场景**        |
| -------------------- | ---------------------------------- | ---------------------------- |
| **理解类任务** | 文本分类、实体识别、问答系统       | 有限（需额外设计prompt）     |
| **生成类任务** | 需配合解码器（如BERT+Decoder结构） | 文本生成、对话系统、代码补全 |
| **零样本学习** | 依赖fine-tuning                    | GPT-3展现强大零样本能力      |
| **推理效率**   | 可并行计算全序列                   | 自回归导致推理时延较高       |

**关键洞见**

1. **信息流方向决定能力边界**BERT的双向性使其擅长捕捉深层语义关系（如指代消解），而GPT的单向性使其天然适配序列生成
2. **预训练任务塑造认知方式**MLM让BERT学习"填空式"推理，自回归训练使GPT掌握"渐进式"创作
3. **工程实践中的趋同演化**

   - 最新模型（如PaLM、GLM）尝试结合双向和单向优势
   - 位置编码方案逐渐统一为可学习参数
   - 缩放定律（Scaling Laws）推动模型参数爆炸式增长
4. **微调范式的分野**  

   | **范式** | **BERT**     | **GPT**       |
   | -------------- | ------------------ | ------------------- |
   | 典型微调方式   | 添加任务特定输出层 | Prompt Engineering  |
   | 参数更新       | 通常全参数微调     | 常使用Prompt Tuning |
   | 数据效率       | 小样本表现优秀     | 需大量示例          |

**延伸思考**

- **第三代架构的突破**：BERT-style和GPT-style的界限正在模糊
  - **双向+生成**：UniLM、GLM通过注意力mask控制信息流方向
  - **稀疏注意力**：Longformer、Sparse Transformer突破长度限制
- **多模态演进**：
  - BERT派生的ViLBERT处理视觉-语言任务
  - GPT-4展现跨模态生成能力

这种架构差异的本质反映了人类认知的两个维度：**全面理解**（BERT）与**渐进创造**（GPT），两者的持续融合将推动下一代通用AI的发展。

# 学习工具

* [Transformer动画演示](https://jalammar.github.io/illustrated-transformer/)
* [BERT Feature可视化](https://exbert.net/)
