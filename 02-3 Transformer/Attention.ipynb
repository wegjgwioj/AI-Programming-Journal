{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2KI5fCvbckGd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X3TH1VDzIEE",
        "outputId": "ef650379-d296-42c2-d361-db7dd90f9da8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 18 11:24:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 手写 self-attention\n",
        "## 1 公式\n",
        " $ Attention (Q,K,V) = softmax(\\frac{Q·K^T}{\\sqrt[]d_k})·V$"
      ],
      "metadata": {
        "id": "x0qGFsBQB84D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 基础版本"
      ],
      "metadata": {
        "id": "2KI5fCvbckGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "P3NjiZ5iCbjo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DZqxVh8vB5Mu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5056db71-11bf-46a5-dc1c-dc410416d7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.5258, 0.4742],\n",
            "         [0.5487, 0.4513]],\n",
            "\n",
            "        [[0.5083, 0.4917],\n",
            "         [0.4965, 0.5035]],\n",
            "\n",
            "        [[0.4969, 0.5031],\n",
            "         [0.4967, 0.5033]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[-0.7143, -0.3710, -0.0556, -0.4849],\n",
            "         [-0.7055, -0.3773, -0.0633, -0.4800]],\n",
            "\n",
            "        [[-0.6510, -0.1262,  0.1903, -0.7163],\n",
            "         [-0.6441, -0.1247,  0.1908, -0.7155]],\n",
            "\n",
            "        [[-0.7555, -0.3069, -0.0465, -0.6092],\n",
            "         [-0.7555, -0.3069, -0.0465, -0.6092]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 手动编写selfAttention层（禁止使用库函数）\n",
        "   # v1\n",
        "class selfAttentionV1(nn.Module):\n",
        "  def __init__(self,hidden_dim:int = 728) -> None:\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.query_proj = nn.Linear(hidden_dim,hidden_dim)\n",
        "        self.key_proj = nn.Linear(hidden_dim,hidden_dim)\n",
        "        self.value_proj = nn.Linear(hidden_dim,hidden_dim)\n",
        "  def forward(self,X):\n",
        "\n",
        "     # X shape is :(batch_size,seq_len,hidden_dim)\n",
        "        Q = self.query_proj(X)\n",
        "        K = self.key_proj(X)\n",
        "        V = self.value_proj(X)\n",
        "\n",
        "     # Q K V shape (batch,seq,hidden_dim)\n",
        "\n",
        "     # attention_value is :(batch,seq,seq)\n",
        "        attention_value = torch.matmul(\n",
        "         # K 要变成(batch,hidden_dim,seq)\n",
        "            Q,K.transpose(-1,-2)\n",
        "         )\n",
        "     # (batch,seq,seq)\n",
        "        attention_weight = torch.softmax(\n",
        "         attention_value/math.sqrt(self.hidden_dim),# 防止梯度消失\n",
        "         dim = -1\n",
        "        )\n",
        "        print(attention_weight)\n",
        "     # (batch,seq,hidden_dim)\n",
        "        return torch.matmul(attention_weight,V)\n",
        "\n",
        "X = torch.rand(3,2,4)\n",
        "\n",
        "self_att_net=selfAttentionV1(4)\n",
        "print(self_att_net(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 效率优化"
      ],
      "metadata": {
        "id": "mXp6qyDCYPJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 网络比较小 QKV 合并运算--效率优化  V2\n",
        "class selfAttentionV2(nn.Module):\n",
        "  def __init__(self,dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        self.proj = nn.Linear(dim,dim * 3)\n",
        "  def forward(self,X):\n",
        "    # X\n",
        "    QKV=self.proj(X)\n",
        "    Q,K,V = torch.split(QKV,self.dim,dim = -1)#分开\n",
        "    attention_weight = torch.softmax(\n",
        "         torch.matmul(Q,K.transpose(-1,-2))/math.sqrt(self.dim),\n",
        "         dim = -1\n",
        "        )\n",
        "    output = attention_weight @ V\n",
        "    return output\n",
        "\n",
        "X = torch.rand(3,2,4)\n",
        "\n",
        "net=selfAttentionV2(4)\n",
        "print(net(X))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beUEO_lxcYuH",
        "outputId": "27f7a206-d637-4c1e-a5c7-04f22bffe36e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0857, -0.1794,  0.8006,  0.1887],\n",
            "         [ 0.0854, -0.1785,  0.7998,  0.1885]],\n",
            "\n",
            "        [[ 0.2623, -0.3023,  0.9188,  0.1146],\n",
            "         [ 0.2635, -0.3020,  0.9179,  0.1132]],\n",
            "\n",
            "        [[-0.3038,  0.0788,  0.6206,  0.3794],\n",
            "         [-0.3038,  0.0788,  0.6206,  0.3795]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 加入一些细节"
      ],
      "metadata": {
        "id": "n865w0c6B8EU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.dropout 位置\n",
        "#  2.attention_mask\n",
        "#  3.output 矩阵映射()\n",
        "class selfAttentionV3(nn.Module):\n",
        "  def __init__(self,dim,*args,**kwargs) -> None:\n",
        "        super().__init__(*args,**kwargs)\n",
        "        self.dim = dim\n",
        "\n",
        "        self.proj = nn.Linear(dim,dim * 3)\n",
        "        self.attention_dropout = nn.Dropout(0.1)\n",
        "        self.output_proj = nn.Linear(dim,dim)\n",
        "\n",
        "  def forward(self,X,attention_mask = None):\n",
        "\n",
        "    QKV=self.proj(X)\n",
        "\n",
        "    Q,K,V = torch.split(QKV,self.dim,dim = -1)#分开\n",
        "    attention_weight = Q @ K.transpose(-1,-2)/math.sqrt(self.dim)\n",
        "    if attention_mask is not None:\n",
        "      attention_weight =attention_weight.masked_fill(\n",
        "          attention_mask == 0,\n",
        "          float(\"-1e20\"))\n",
        "    attention_weight = torch.softmax(attention_weight,dim = -1)\n",
        "    attention_weight = self.attention_dropout(attention_weight)\n",
        "    output = attention_weight @ V\n",
        "    output = self.output_proj(output) #output 矩阵映射\n",
        "    return output\n",
        "\n",
        "\n",
        "X=torch.rand(3,4,2)\n",
        "mark=torch.tensor(\n",
        "    [\n",
        "    [1,1,1,0],\n",
        "    [1,1,0,0],\n",
        "    [1,0,0,0]\n",
        "    ])\n",
        "mark=mark.unsqueeze(dim=1).repeat(1,4,1)\n",
        "print(f\"repeat shape:{mark.size()}\")\n",
        "\n",
        "net=selfAttentionV3(2)\n",
        "print(net(X,attention_mask=mark))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHBDzpyNqC8Y",
        "outputId": "daadd549-d5ac-47f9-8165-1a7efa14d13c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repeat shape:torch.Size([3, 4, 4])\n",
            "tensor([[[-0.1209, -0.3495],\n",
            "         [-0.1172, -0.3238],\n",
            "         [-0.1174, -0.3492],\n",
            "         [-0.1183, -0.3237]],\n",
            "\n",
            "        [[-0.3265, -0.3805],\n",
            "         [-0.3371, -0.3820],\n",
            "         [-0.3314, -0.3812],\n",
            "         [-0.3241, -0.3319]],\n",
            "\n",
            "        [[-0.3254, -0.3647],\n",
            "         [-0.3254, -0.3647],\n",
            "         [-0.3254, -0.3647],\n",
            "         [-0.3254, -0.3647]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 面试写法\n"
      ],
      "metadata": {
        "id": "Q1GBQ4f_B7ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class selfAttentionInterview(nn.Module):\n",
        "  def __init__(self,dim) -> None:\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        self.query = nn.Linear(dim,dim)\n",
        "        self.key = nn.Linear(dim,dim)\n",
        "        self.value = nn.Linear(dim,dim)\n",
        "\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(0.1)\n",
        "  def forward(self,X,attention_mask = None):\n",
        "    Q = self.query(X)\n",
        "    K = self.key(X)\n",
        "    V = self.value(X)\n",
        "\n",
        "    attention_weight= Q @ K.transpose(-1,-2)/math.sqrt(self.dim)\n",
        "    if attention_mask is not None:\n",
        "      attention_weight=attention_weight.masked_fill(\n",
        "          attention_mask == 0,\n",
        "          float(\"-inf\")\n",
        "      )\n",
        "    attention_weight = torch.softmax(attention_weight,dim = -1)\n",
        "    print(attention_weight)#验证\n",
        "    attention_weight = self.attention_dropout(attention_weight)\n",
        "    output = attention_weight @ V\n",
        "    return output\n",
        "\n",
        "X = torch.rand(3,4,2)\n",
        "mark = torch.tensor(\n",
        "    [\n",
        "    [1,1,1,0],\n",
        "    [1,1,0,0],\n",
        "    [1,0,0,0]\n",
        "    ])\n",
        "mark = mark.unsqueeze(dim=1).repeat(1,4,1)\n",
        "net = selfAttentionInterview(2)\n",
        "print(net(X,attention_mask=mark))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZy3hJW6ufNj",
        "outputId": "1c497f63-bee1-4a31-8d80-04ceaeeba010"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3487, 0.3186, 0.3327, 0.0000],\n",
            "         [0.3655, 0.3010, 0.3335, 0.0000],\n",
            "         [0.3527, 0.3142, 0.3331, 0.0000],\n",
            "         [0.3653, 0.3008, 0.3340, 0.0000]],\n",
            "\n",
            "        [[0.5355, 0.4645, 0.0000, 0.0000],\n",
            "         [0.5609, 0.4391, 0.0000, 0.0000],\n",
            "         [0.5380, 0.4620, 0.0000, 0.0000],\n",
            "         [0.5385, 0.4615, 0.0000, 0.0000]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[[ 0.0567, -0.4883],\n",
            "         [ 0.0519, -0.4861],\n",
            "         [ 0.0307, -0.3295],\n",
            "         [ 0.0519, -0.4861]],\n",
            "\n",
            "        [[ 0.6038, -0.5999],\n",
            "         [ 0.5965, -0.5966],\n",
            "         [ 0.3495, -0.3087],\n",
            "         [ 0.3491, -0.3083]],\n",
            "\n",
            "        [[ 0.5236, -0.5632],\n",
            "         [ 0.5236, -0.5632],\n",
            "         [ 0.5236, -0.5632],\n",
            "         [ 0.5236, -0.5632]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}