# 大模型服务吞吐率 太小？（三重境界）

$$
吞吐率 = \frac{处理的请求N}{延时}
$$

在一定时间内，处理的请求数 除以 消耗的时间

1. 尽量降低模型的推理延迟   同时 增加 模型并行处理请求的能力

* 模型的 forward 时间：尽可能小
* 处理的请求数
  * 模型一次处理的条数： batch_size
  * 服务的实例数量：部署了多少个节点 k * n

2. * 单次推理延迟  （1） 权重+激活量化 （2）profile到服务的计算资源还有空余的话：eg GPU利用率还有60% “ 投机采样”=小模型猜测+大模型验证 （3）提高 访存利用率 +所使用模型的计算量来分析  若不是计算密集型（计算速度>访存速度）-->内存访问的瓶颈 （self-Attention 大部分如此）--->减少访存次数--->flash-Attention
   * 增加 模型并行处理请求的能力 (1)显存运行-->增大batch_size {静态batching、动态、连续\}  （2）业务条件允许-->水平扩展、增加节点数  ：scaling 问题 最大化利用每个结点的运算能力--负载均衡 eg k8s+GPU利用率监测+QLB负载均衡进行调优
3. 最适合落地：模型并行处理请求的能力 影响最大
   增大batch_size , 会以10%--30%的单次时延增加，-->**continuous batching** 技术：一旦批中一个序列完成生成，就在其位置可以插入新序列，若是流式输出（利用异步协程async 和await实现计算资源切换，tokenize和detokenize 也可以用异步协程实现，提高编解码）

# 如何解决大模型中的badecase

实际业务！！

1.加前置模块（例如敏感话题：拒识模块）

2.加后处理（一个处理模块二次过滤）

3.调prompt（bug不紧急）

4.模型微调优化：在积累一个周期后进行（耗时，且不稳定）

# transformer 的Attention 机制？


# Attention 的 各种区别

# FFN 模块 原理

# 多头注意力计算复杂度 过程

# transformer 推理为何做 kv 缓存？kv 具体怎么做的？


# 显存不足的缓解
